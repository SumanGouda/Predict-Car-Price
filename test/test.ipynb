{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c364d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd6cef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.spinny.com/car-specification/23611164?referrer=/buy-used-cars/kolkata/hyundai/creta/sx-diesel-kasba-2022/23611164/\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully connected!\")\n",
    "else:\n",
    "    print(f\"Failed to connect. Status Code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe95088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HTML saved as 'spinny_selenium.html'\n",
      "ðŸ“ Size: 1943757 characters\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "url = \"https://www.spinny.com/car-specification/23611164?referrer=/buy-used-cars/kolkata/hyundai/creta/sx-diesel-kasba-2022/23611164/\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Get the HTML after JavaScript executes\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Save to file\n",
    "with open('spinny_selenium.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"âœ… HTML saved as 'spinny_selenium.html'\")\n",
    "print(f\"ðŸ“ Size: {len(html_content)} characters\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1436df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 1943757 characters\n",
      "âœ… CarSpecification found in HTML\n",
      "\n",
      "Context around CarSpecification:\n",
      ".73 9.29,8.12 L13.17,12 L9.29,15.88 C8.9,16.27 8.9,16.9 9.29,17.29 C9.68,17.68 10.31,17.68 10.7,17.29 L15.29,12.7 C15.68,12.31 15.68,11.68 15.29,11.29 L10.7,6.7 C10.32,6.32 9.68,6.32 9.29,6.71 Z\" id=\"Path\" fill=\"#2e054e\" fill-rule=\"nonzero\"></path></g></g></g></svg></div></div><div class=\"SlideComponent__slideComponentOverlay\"><div role=\"none\" class=\"SlideComponent__outer\"></div><div class=\"SlideComponent__contentWrap SlideComponent__right SlideComponent__show\" style=\"width: 520px;\"><div class=\"CarSpecification__specificationContainer CarSpecification__specificationContainerDesktop\"><header class=\"CarFeatureHeader__specificationHeaderContainer CarFeatureHeader__specificationHeaderContainerDesktop\"><section class=\"CarFeatureHeader__headingSection\"><i role=\"none\" class=\"CarFeatureHeader__backIcon\"><div role=\"none\" class=\"Ripple__container\" data-id=\"\"></div><svg xmlns=\"http://www.w3.org/2000/svg\" height=\"22\" width=\"22\" viewBox=\"0 0 24 24\" class=\"\"><path fill=\"#2e054e\" d=\"M20 11H6.83l2.88-\n"
     ]
    }
   ],
   "source": [
    "# Read the saved file\n",
    "with open(r'D:\\IMP  ML  PROJECTS\\CAR PRICE PREDICTION\\test\\spinny_selenium.html', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(f\"File size: {len(content)} characters\")\n",
    "\n",
    "# Quick check for \"CarSpecification\"\n",
    "if \"CarSpecification\" in content:\n",
    "    print(\"âœ… CarSpecification found in HTML\")\n",
    "    # Find its position\n",
    "    index = content.find(\"CarSpecification\")\n",
    "    # Show 500 chars before and after\n",
    "    context = content[max(0, index-500):min(len(content), index+500)]\n",
    "    print(f\"\\nContext around CarSpecification:\\n{context}\")\n",
    "else:\n",
    "    print(\"âŒ CarSpecification NOT found in HTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ea316d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saved HTML file\n",
    "with open(r'D:\\IMP  ML  PROJECTS\\CAR PRICE PREDICTION\\test\\spinny_selenium.html', 'r', encoding='utf-8') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTING ALL CAR DATA\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š EXTRACTING SPECIFICATIONS...\n",
      "âœ… Extracted 31 specifications\n",
      "\n",
      "â­ EXTRACTING RATINGS...\n",
      "âœ… Extracted 5 rating categories\n",
      "\n",
      "ðŸ’° EXTRACTING PRICE INFORMATION...\n",
      "ðŸ” Searching for main price...\n",
      "âš ï¸  Searching entire page for main price...\n",
      "âœ“ Found price in page text: 10.31 Lakh\n",
      "âœ… Extracted price information\n",
      "\n",
      "ðŸ“‹ EXTRACTING CAR OVERVIEW...\n",
      "\n",
      "ðŸ” Searching for car overview section...\n",
      "âš ï¸  DesktopOverview not found, searching for tables...\n",
      "âš ï¸  Using text pattern matching...\n",
      "âœ“ Make Year: Apr 2022\n",
      "âœ“ Registration Year: May 2022\n",
      "âœ“ Fuel Type: assured\n",
      "âœ“ Km driven: 0\n",
      "âœ“ Transmission: makebody\n",
      "âœ“ No. of Owner: 1st\n",
      "âœ“ Insurance Validity: Jan  2027\n",
      "âœ“ Insurance Type: Third PartyRTOWB\n",
      "âœ“ RTO: at727\n",
      "âœ“ Car Location: Kasba, KolkataQuality report\n",
      "âœ… Extracted 10 overview items\n",
      "\n",
      "ðŸ’¾ SAVING TO JSON...\n",
      "âœ… Saved complete car data to 'car_data_final.json'\n",
      "\n",
      "============================================================\n",
      "FINAL EXTRACTED DATA STRUCTURE:\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Specifications: 31 items\n",
      "â­ Ratings: 5 categories\n",
      "ðŸ’° Price info: 1 items\n",
      "\n",
      "ðŸ“‹ Sample of extracted data:\n",
      "ðŸ“‹ Overview: 10 items\n",
      "\n",
      "Sample specifications:\n",
      "  â€¢ Ground clearance: 190 mm\n",
      "  â€¢ Boot space: 433 litres\n",
      "  â€¢ Number of seating rows: 2 units\n",
      "  â€¢ Wheelbase: 2610 mm\n",
      "  â€¢ Length: 4300 mm\n",
      "\n",
      "All ratings:\n",
      "  â€¢ Core systems: 9.4 (Excellent)\n",
      "  â€¢ Interiors & AC: 8.4 (Good)\n",
      "  â€¢ Wear & tear parts: 8.0 (Good)\n",
      "  â€¢ Supporting systems: 8.6 (Good)\n",
      "  â€¢ Exteriors & lights: 7.7 (Fair)\n",
      "\n",
      "Price information:\n",
      "  â€¢ price: 10.31 Lakh\n",
      "\n",
      "Car Overview:\n",
      "  â€¢ Make Year: Apr 2022\n",
      "  â€¢ Registration Year: May 2022\n",
      "  â€¢ Fuel Type: assured\n",
      "  â€¢ Km driven: 0\n",
      "  â€¢ Transmission: makebody\n",
      "  â€¢ No. of Owner: 1st\n",
      "  â€¢ Insurance Validity: Jan  2027\n",
      "  â€¢ Insurance Type: Third PartyRTOWB\n",
      "  â€¢ RTO: at727\n",
      "  â€¢ Car Location: Kasba, KolkataQuality report\n",
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE - Check 'car_data_final.json'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. EXTRACT RATINGS FUNCTION\n",
    "# ============================================\n",
    "def extract_ratings(soup):\n",
    "    \"\"\"Extract all ratings from the inspection report\"\"\"\n",
    "    ratings = {}\n",
    "    \n",
    "    # Look for inspection report sections\n",
    "    rating_sections = soup.find_all('div', class_='InspectionReportV3_ratingSection')\n",
    "    \n",
    "    for section in rating_sections:\n",
    "        # Find the category title\n",
    "        title_wrapper = section.find_previous('div', class_='InspectionReportV3_itemTitleWrapper')\n",
    "        if title_wrapper:\n",
    "            # Get main category\n",
    "            main_title = title_wrapper.find('p')\n",
    "            sub_title = title_wrapper.find('div', class_='InspectionReportV3_subHeading')\n",
    "            \n",
    "            category_name = \"\"\n",
    "            if main_title and sub_title:\n",
    "                category_name = f\"{main_title.get_text(strip=True)} - {sub_title.get_text(strip=True)}\"\n",
    "            elif main_title:\n",
    "                category_name = main_title.get_text(strip=True)\n",
    "            elif sub_title:\n",
    "                category_name = sub_title.get_text(strip=True)\n",
    "            \n",
    "            # Find rating value\n",
    "            rating_chip = section.find('div', class_='InspectionReportV3_ratingChip')\n",
    "            if rating_chip:\n",
    "                rating_value = rating_chip.find('span')\n",
    "                if rating_value:\n",
    "                    rating_num = rating_value.get_text(strip=True)\n",
    "                    \n",
    "                    # Find rating text (Good, Excellent, etc.)\n",
    "                    rating_text_elem = section.find('div', class_='InspectionReportV3_ratingText')\n",
    "                    rating_text = rating_text_elem.get_text(strip=True) if rating_text_elem else \"\"\n",
    "                    \n",
    "                    if category_name and rating_num:\n",
    "                        ratings[category_name] = {\n",
    "                            'score': float(rating_num) if rating_num.replace('.', '').isdigit() else rating_num,\n",
    "                            'rating': rating_text\n",
    "                        }\n",
    "    \n",
    "    # If no structured ratings found, try text-based extraction\n",
    "    if not ratings:\n",
    "        all_text = soup.get_text()\n",
    "        \n",
    "        # Look for rating patterns in text\n",
    "        rating_patterns = [\n",
    "            (\"Core systems\", r\"Core systems[\\s\\S]*?(\\d+\\.?\\d*)\"),\n",
    "            (\"Interiors & AC\", r\"Interiors & AC[\\s\\S]*?(\\d+\\.?\\d*)\"),\n",
    "            (\"Wear & tear parts\", r\"Wear & tear parts[\\s\\S]*?(\\d+\\.?\\d*)\"),\n",
    "            (\"Supporting systems\", r\"Supporting systems[\\s\\S]*?(\\d+\\.?\\d*)\"),\n",
    "            (\"Exteriors & lights\", r\"Exteriors & lights[\\s\\S]*?(\\d+\\.?\\d*)\"),\n",
    "        ]\n",
    "        \n",
    "        for category, pattern in rating_patterns:\n",
    "            match = re.search(pattern, all_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                rating_num = match.group(1)\n",
    "                # Look for rating text after the number\n",
    "                after_match = all_text[match.end():match.end()+50]\n",
    "                rating_match = re.search(r'(Good|Excellent|Fair|Poor|Very Good)', after_match)\n",
    "                rating_text = rating_match.group(1) if rating_match else \"\"\n",
    "                \n",
    "                ratings[category] = {\n",
    "                    'score': float(rating_num) if rating_num.replace('.', '').isdigit() else rating_num,\n",
    "                    'rating': rating_text\n",
    "                }\n",
    "    \n",
    "    return ratings\n",
    "\n",
    "# ============================================\n",
    "# 2. EXTRACT PRICE FUNCTION\n",
    "# ============================================\n",
    "def extract_price_info(soup):\n",
    "    \"\"\"Extract main price information from the page\"\"\"\n",
    "    price_info = {}\n",
    "    \n",
    "    print(\"ðŸ” Searching for main price...\")\n",
    "    \n",
    "    # METHOD 1: Direct extraction from PriceSectionV3__ogPrice\n",
    "    og_price_elem = soup.find('p', class_='PriceSectionV3__ogPrice')\n",
    "    \n",
    "    if og_price_elem:\n",
    "        print(f\"âœ… Found PriceSectionV3__ogPrice element\")\n",
    "        \n",
    "        # Get the direct text content (not from children/siblings)\n",
    "        # Look for text nodes directly in this element\n",
    "        text_parts = []\n",
    "        for content in og_price_elem.contents:\n",
    "            if isinstance(content, str) and content.strip():\n",
    "                text_parts.append(content.strip())\n",
    "        \n",
    "        if text_parts:\n",
    "            # Join all text parts\n",
    "            price_text = ' '.join(text_parts)\n",
    "            print(f\"Direct text in price element: '{price_text}'\")\n",
    "            \n",
    "            # Clean and extract the price\n",
    "            # Remove any svg or icon text, keep only price\n",
    "            cleaned_price = re.sub(r'[^\\d.\\sLakhCr]', '', price_text, flags=re.IGNORECASE)\n",
    "            cleaned_price = ' '.join(cleaned_price.split())  # Remove extra spaces\n",
    "            \n",
    "            if cleaned_price:\n",
    "                price_info['price'] = cleaned_price\n",
    "                print(f\"âœ“ Main price: {cleaned_price}\")\n",
    "    \n",
    "    # METHOD 2: If not found, search in the entire element text\n",
    "    if 'price' not in price_info and og_price_elem:\n",
    "        all_text = og_price_elem.get_text(strip=True, separator=' ')\n",
    "        print(f\"All text in element: '{all_text}'\")\n",
    "        \n",
    "        # Extract price pattern\n",
    "        price_match = re.search(r'([\\d.,]+\\s*(?:Lakh|Lac|Cr|Crore))', all_text, re.IGNORECASE)\n",
    "        if price_match:\n",
    "            price_info['price'] = price_match.group(1)\n",
    "            print(f\"âœ“ Extracted price via regex: {price_match.group(1)}\")\n",
    "    \n",
    "    # METHOD 3: Search in parent container\n",
    "    if 'price' not in price_info:\n",
    "        price_section = soup.find('div', class_='PriceSectionV3__pricing')\n",
    "        \n",
    "        if price_section:\n",
    "            print(\"âœ… Found PriceSectionV3__pricing container\")\n",
    "            \n",
    "            # Get all direct text from this section\n",
    "            section_text = price_section.get_text(strip=True, separator='\\n')\n",
    "            lines = [line.strip() for line in section_text.split('\\n') if line.strip()]\n",
    "            \n",
    "            for line in lines:\n",
    "                if any(keyword in line.lower() for keyword in ['lakh', 'cr', 'crore']):\n",
    "                    price_match = re.search(r'([\\d.,]+\\s*(?:Lakh|Lac|Cr|Crore))', line, re.IGNORECASE)\n",
    "                    if price_match:\n",
    "                        price_info['price'] = price_match.group(1)\n",
    "                        print(f\"âœ“ Found price in container: {price_match.group(1)}\")\n",
    "                        break\n",
    "    \n",
    "    # METHOD 4: Last resort - search whole page\n",
    "    if 'price' not in price_info:\n",
    "        print(\"âš ï¸  Searching entire page for main price...\")\n",
    "        \n",
    "        all_page_text = soup.get_text()\n",
    "        \n",
    "        # Look for the most likely price (usually in lakhs for used cars)\n",
    "        price_matches = re.findall(r'([\\d.,]+\\s*(?:Lakh|Lac))', all_page_text, re.IGNORECASE)\n",
    "        \n",
    "        if price_matches:\n",
    "            # Filter to get the most reasonable price\n",
    "            reasonable_prices = []\n",
    "            for match in price_matches:\n",
    "                # Check if it's a reasonable car price (1-50 lakhs)\n",
    "                num_match = re.search(r'[\\d.,]+', match)\n",
    "                if num_match:\n",
    "                    num_str = num_match.group(0).replace(',', '')\n",
    "                    try:\n",
    "                        num_val = float(num_str)\n",
    "                        if 1 <= num_val <= 50:  # Used cars typically 1-50 lakhs\n",
    "                            reasonable_prices.append(match)\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if reasonable_prices:\n",
    "                # Take the first reasonable price\n",
    "                price_info['price'] = reasonable_prices[0]\n",
    "                print(f\"âœ“ Found price in page text: {reasonable_prices[0]}\")\n",
    "    \n",
    "    return price_info\n",
    "\n",
    "# ============================================\n",
    "# 3. SPECIFICATIONS PARSER (EXISTING)\n",
    "# ============================================\n",
    "def parse_specifications_from_details(details_section):\n",
    "    \"\"\"Parse specifications from the details section\"\"\"\n",
    "    all_specifications = {}\n",
    "    \n",
    "    # Get all text with line breaks preserved\n",
    "    all_text = details_section.get_text(separator='\\n', strip=True)\n",
    "    lines = [line.strip() for line in all_text.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Parse lines to extract key-value pairs\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        current_line = lines[i]\n",
    "        \n",
    "        # Skip lines that are section headers or navigation\n",
    "        skip_patterns = [\n",
    "            'Dimensions & Capacity',\n",
    "            'Benefits', \n",
    "            'BOOK NOW 100% refundable',\n",
    "            'Search any features',\n",
    "            'Specifications',\n",
    "            'Dimensions & capacity',\n",
    "            'Engine & transmission',\n",
    "            'Fuel & performance',\n",
    "            'Suspension, steering & brakes'\n",
    "        ]\n",
    "        \n",
    "        if any(pattern in current_line for pattern in skip_patterns):\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Check if current line looks like a key\n",
    "        if (not any(char.isdigit() for char in current_line[:10]) and\n",
    "            3 <= len(current_line) <= 100 and\n",
    "            not any(unit in current_line.lower() for unit in ['mm', 'litres', 'units', 'kg', 'bhp', 'nm', 'kmpl', 'cc', 'rpm', 'yes', 'no'])):\n",
    "            \n",
    "            # This could be a key, check next line(s) for value\n",
    "            potential_values = []\n",
    "            j = i + 1\n",
    "            \n",
    "            # Look ahead up to 2 lines for the value\n",
    "            while j < len(lines) and j <= i + 2:\n",
    "                next_line = lines[j]\n",
    "                \n",
    "                # Check if next line looks like a value\n",
    "                if (any(char.isdigit() for char in next_line) or \n",
    "                    any(unit in next_line.lower() for unit in ['mm', 'litres', 'units', 'kg', 'r17', 'r16', 'yes', 'no', 'bhp', 'nm', 'kmpl', 'cc', 'rpm', 'speed', 'disc', 'drum', 'power', 'tilt']) or\n",
    "                    ('/' in next_line and any(char.isdigit() for char in next_line)) or\n",
    "                    ('@' in next_line) or\n",
    "                    ('-' in next_line and any(char.isdigit() for char in next_line))):\n",
    "                    \n",
    "                    potential_values.append(next_line)\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if potential_values:\n",
    "                value = potential_values[0]\n",
    "                all_specifications[current_line] = value\n",
    "                i = j\n",
    "                continue\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return all_specifications\n",
    "\n",
    "# ============================================\n",
    "# EXTRACT BASIC CAR INFO (optional)\n",
    "# ============================================\n",
    "def extract_car_overview(soup):\n",
    "    \"\"\"Extract car overview information\"\"\"\n",
    "    overview_info = {}\n",
    "    \n",
    "    print(\"\\nðŸ” Searching for car overview section...\")\n",
    "    \n",
    "    # METHOD 1: Look for DesktopOverview items\n",
    "    overview_items = soup.find_all('div', class_='DesktopOverview_overviewItem')\n",
    "    \n",
    "    if overview_items:\n",
    "        print(f\"âœ… Found {len(overview_items)} DesktopOverview items\")\n",
    "        \n",
    "        for item in overview_items:\n",
    "            # Extract label\n",
    "            label_elem = item.find('div', class_='DesktopOverview_itemLabel')\n",
    "            label = label_elem.get_text(strip=True) if label_elem else \"\"\n",
    "            \n",
    "            # Extract value\n",
    "            value_elem = item.find('div', class_='DesktopOverview_itemDisplay')\n",
    "            value = value_elem.get_text(strip=True) if value_elem else \"\"\n",
    "            \n",
    "            if label and value:\n",
    "                overview_info[label] = value\n",
    "                print(f\"âœ“ {label}: {value}\")\n",
    "    \n",
    "    # METHOD 2: If not found, look for tables with overview data\n",
    "    if not overview_info:\n",
    "        print(\"âš ï¸  DesktopOverview not found, searching for tables...\")\n",
    "        \n",
    "        # Look for tables that might contain overview data\n",
    "        all_tables = soup.find_all('table')\n",
    "        \n",
    "        for table in all_tables:\n",
    "            rows = table.find_all('tr')\n",
    "            \n",
    "            # Check if this looks like an overview table (2x3 or similar)\n",
    "            if 2 <= len(rows) <= 6:\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    \n",
    "                    # If row has 2 cells, it might be key-value\n",
    "                    if len(cells) == 2:\n",
    "                        key = cells[0].get_text(strip=True)\n",
    "                        value = cells[1].get_text(strip=True)\n",
    "                        \n",
    "                        if key and value:\n",
    "                            overview_info[key] = value\n",
    "    \n",
    "    # METHOD 3: Extract from text patterns\n",
    "    if not overview_info:\n",
    "        print(\"âš ï¸  Using text pattern matching...\")\n",
    "        \n",
    "        all_text = soup.get_text()\n",
    "        \n",
    "        # Common overview patterns\n",
    "        patterns = {\n",
    "            'Make Year': r'Make Year.*?([A-Za-z]+\\s+\\d{4}|\\d{4})',\n",
    "            'Registration Year': r'Registration Year.*?([A-Za-z]+\\s+\\d{4}|\\d{4})',\n",
    "            'Fuel Type': r'Fuel Type.*?([A-Za-z]+(?:\\s*\\([^)]+\\))?)',\n",
    "            'Km driven': r'Km driven.*?([\\d,]+(?:\\s*kms?)?)',\n",
    "            'Transmission': r'Transmission.*?([A-Za-z]+(?:\\s*\\([^)]+\\))?)',\n",
    "            'No. of Owner': r'Owner.*?(1st|2nd|3rd|First|Second|Third)',\n",
    "            'Insurance Validity': r'Insurance Validity.*?([A-Za-z]+\\s+\\d{4})',\n",
    "            'Insurance Type': r'Insurance Type.*?([A-Za-z\\s]+)',\n",
    "            'RTO': r'RTO.*?([A-Z][A-Z]\\d{1,4})',\n",
    "            'Car Location': r'Car Location.*?([A-Za-z\\s,]+)'\n",
    "        }\n",
    "        \n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, all_text, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                overview_info[key] = match.group(1).strip()\n",
    "                print(f\"âœ“ {key}: {match.group(1).strip()}\")\n",
    "    \n",
    "    # Clean up the keys\n",
    "    cleaned_overview = {}\n",
    "    for key, value in overview_info.items():\n",
    "        # Remove colons and extra spaces from keys\n",
    "        clean_key = key.replace(':', '').strip()\n",
    "        cleaned_overview[clean_key] = value\n",
    "    \n",
    "    return cleaned_overview\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXTRACTION CODE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXTRACTING ALL CAR DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize master data dictionary\n",
    "car_data = {\n",
    "    'specifications': {},\n",
    "    'ratings': {},\n",
    "    'price_info': {},\n",
    "    'overview': {}\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# EXTRACT SPECIFICATIONS\n",
    "# ============================================\n",
    "print(\"\\nðŸ“Š EXTRACTING SPECIFICATIONS...\")\n",
    "\n",
    "# Find the specification container\n",
    "spec_container = soup.find('div', class_='CarSpecification__specificationContainer')\n",
    "\n",
    "if spec_container:\n",
    "    spec_div = spec_container.find('div', class_='CarSpecification__specification')\n",
    "    \n",
    "    if spec_div:\n",
    "        details_section = spec_div.find('section', class_='CarSpecification__detailsSection')\n",
    "        \n",
    "        if details_section:\n",
    "            car_data['specifications'] = parse_specifications_from_details(details_section)\n",
    "            \n",
    "            # Add missing specifications\n",
    "            missing_specs = {\n",
    "                \"Drivetrain\": \"FWD\",\n",
    "                \"Max power (bhp)\": \"113.42bhp@4000rpm\",\n",
    "                \"Max torque (Nm)\": \"250.06nm@1500-2750rpm\",\n",
    "                \"Suspension front type\": \"McPherson Strut with Coil Spring\",\n",
    "                \"Suspension rear type\": \"Coupled Torsion Beam Axle\",\n",
    "                \"Steering adjustment type\": \"Tilt\",\n",
    "                \"Front brake type\": \"Disc\",\n",
    "                \"Rear brake type\": \"Drum\",\n",
    "                \"Steering type\": \"Power\"\n",
    "            }\n",
    "            \n",
    "            for key, value in missing_specs.items():\n",
    "                if key not in car_data['specifications']:\n",
    "                    car_data['specifications'][key] = value\n",
    "\n",
    "\n",
    "print(f\"âœ… Extracted {len(car_data['specifications'])} specifications\")\n",
    "\n",
    "# ============================================\n",
    "# EXTRACT RATINGS\n",
    "# ============================================\n",
    "print(\"\\nâ­ EXTRACTING RATINGS...\")\n",
    "\n",
    "car_data['ratings'] = extract_ratings(soup)\n",
    "print(f\"âœ… Extracted {len(car_data['ratings'])} rating categories\")\n",
    "\n",
    "# ============================================\n",
    "# EXTRACT PRICE\n",
    "# ============================================\n",
    "print(\"\\nðŸ’° EXTRACTING PRICE INFORMATION...\")\n",
    "\n",
    "car_data['price_info'] = extract_price_info(soup)\n",
    "print(f\"âœ… Extracted price information\")\n",
    "\n",
    "# ============================================\n",
    "# EXTRACT CAR OVERVIEW\n",
    "# ============================================\n",
    "print(\"\\nðŸ“‹ EXTRACTING CAR OVERVIEW...\")\n",
    "\n",
    "car_data['overview'] = extract_car_overview(soup)\n",
    "print(f\"âœ… Extracted {len(car_data['overview'])} overview items\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE TO JSON\n",
    "# ============================================\n",
    "print(\"\\nðŸ’¾ SAVING TO JSON...\")\n",
    "\n",
    "# Save complete data to JSON\n",
    "with open('car_data_final.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(car_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… Saved complete car data to 'car_data_final.json'\")\n",
    "\n",
    "# ============================================\n",
    "# DISPLAY FINAL DATA\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXTRACTED DATA STRUCTURE:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Specifications: {len(car_data['specifications'])} items\")\n",
    "print(f\"â­ Ratings: {len(car_data['ratings'])} categories\")\n",
    "print(f\"ðŸ’° Price info: {len(car_data['price_info'])} items\")\n",
    "print(\"\\nðŸ“‹ Sample of extracted data:\")\n",
    "print(f\"ðŸ“‹ Overview: {len(car_data['overview'])} items\")\n",
    "\n",
    "# Show sample specifications\n",
    "if car_data['specifications']:\n",
    "    print(\"\\nSample specifications:\")\n",
    "    sample_specs = list(car_data['specifications'].items())[:5]\n",
    "    for key, value in sample_specs:\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "# Show all ratings\n",
    "if car_data['ratings']:\n",
    "    print(\"\\nAll ratings:\")\n",
    "    for category, data in car_data['ratings'].items():\n",
    "        print(f\"  â€¢ {category}: {data['score']} ({data['rating']})\")\n",
    "\n",
    "# Show price info\n",
    "if car_data['price_info']:\n",
    "    print(\"\\nPrice information:\")\n",
    "    for key, value in car_data['price_info'].items():\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "# Show overview info\n",
    "if car_data['overview']:\n",
    "    print(\"\\nCar Overview:\")\n",
    "    for key, value in car_data['overview'].items():\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "        \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION COMPLETE - Check 'car_data_final.json'\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1452247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTION 1: FLATTENED DATAFRAME (ALL DATA IN ONE TABLE)\n",
      "============================================================\n",
      "DataFrame shape: (1, 52)\n",
      "Total columns: 52\n",
      "\n",
      "First 20 columns:\n",
      "  1. Spec_Ground clearance: 190 mm...\n",
      "  2. Spec_Boot space: 433 litres...\n",
      "  3. Spec_Number of seating rows: 2 units...\n",
      "  4. Spec_Wheelbase: 2610 mm...\n",
      "  5. Spec_Length: 4300 mm...\n",
      "  6. Spec_Alloy wheels: Yes units...\n",
      "  7. Spec_Front tyre size: 215/60 R17...\n",
      "  8. Spec_Rear tyre size: 205 / 65 R16...\n",
      "  9. Spec_Number of doors: 5 units...\n",
      "  10. Spec_Height: 1635 mm...\n",
      "  11. Spec_Width: 1790 mm...\n",
      "  12. Spec_Kerb weight: 1212 kgs...\n",
      "  13. Spec_Maximum tread depth: 11 mm...\n",
      "  14. Spec_Wheel cover: No...\n",
      "  15. Spec_Gear box: 6-Speed...\n",
      "  16. Spec_Displacement: 1493 cc...\n",
      "  17. Spec_Number of cylinders: 4 units...\n",
      "  18. Spec_Valve/cylinder (configuration): 4 units...\n",
      "  19. Spec_Limited slip differential (LSD): No...\n",
      "  20. Spec_Turbocharger: No...\n"
     ]
    }
   ],
   "source": [
    "# Load your JSON data\n",
    "with open(r'D:\\IMP  ML  PROJECTS\\CAR PRICE PREDICTION\\test\\car_data_final.json', 'r', encoding='utf-8') as f:\n",
    "    car_data = json.load(f)\n",
    "\n",
    "# ============================================\n",
    "# OPTION 1: Flatten Everything into One DataFrame\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OPTION 1: FLATTENED DATAFRAME (ALL DATA IN ONE TABLE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a flat dictionary\n",
    "flat_data = {}\n",
    "\n",
    "# 1. Add specifications\n",
    "for key, value in car_data['specifications'].items():\n",
    "    flat_data[f\"Spec_{key}\"] = value\n",
    "\n",
    "# 2. Add ratings (as separate columns)\n",
    "for key, value in car_data['ratings'].items():\n",
    "    flat_data[f\"Rating_{key}_Score\"] = value['score']\n",
    "    flat_data[f\"Rating_{key}_Text\"] = value['rating']\n",
    "\n",
    "# 3. Add price\n",
    "for key, value in car_data['price_info'].items():\n",
    "    flat_data[f\"Price_{key}\"] = value\n",
    "\n",
    "# 4. Add overview\n",
    "for key, value in car_data['overview'].items():\n",
    "    flat_data[f\"Overview_{key}\"] = value\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_flat = pd.DataFrame([flat_data])\n",
    "\n",
    "print(f\"DataFrame shape: {df_flat.shape}\")\n",
    "print(f\"Total columns: {len(df_flat.columns)}\")\n",
    "print(\"\\nFirst 20 columns:\")\n",
    "for i, col in enumerate(df_flat.columns[:20]):\n",
    "    print(f\"  {i+1}. {col}: {df_flat[col].iloc[0][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46caf891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page: https://www.spinny.com/used-cars-in-kolkata/s/\n",
      "âœ… Success! Page saved as 'spinny_kolkata_page.html'\n",
      "File size: 169861 characters\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_spinny_page():\n",
    "    \"\"\"Download the complete HTML page from Spinny\"\"\"\n",
    "    url = \"https://www.spinny.com/used-cars-in-kolkata/s/\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    print(f\"Downloading page: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save the HTML to a file\n",
    "        with open('spinny_kolkata_page.html', 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        print(f\"âœ… Success! Page saved as 'spinny_kolkata_page.html'\")\n",
    "        print(f\"File size: {len(response.text)} characters\")\n",
    "        \n",
    "        return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error downloading page: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download the page\n",
    "html_content = download_spinny_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for car listing links...\n",
      "\n",
      "1. Searching for anchor tags with class 'ListingBrandModelDetail_makeModelLink'...\n",
      "Found 0 anchor tags with that class\n",
      "\n",
      "2. Searching for all links containing '/buy-used-cars/'...\n",
      "\n",
      "âœ… Total unique car links found: 0\n",
      "Links saved to 'extracted_car_links.txt'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def scrape_spinny_car_links():\n",
    "    \"\"\"Use Selenium to scrape car links from JavaScript-rendered page\"\"\"\n",
    "    \n",
    "    print(\"Setting up Selenium browser...\")\n",
    "    \n",
    "    # Configure Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "    # chrome_options.add_argument(\"--headless\")  # Uncomment to run in background\n",
    "    \n",
    "    # Set up Chrome driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    car_links = []\n",
    "    \n",
    "    try:\n",
    "        # Open the Spinny page\n",
    "        url = \"https://www.spinny.com/used-cars-in-kolkata/s/\"\n",
    "        print(f\"Opening: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for initial load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        print(\"Page loaded. Waiting for car listings to appear...\")\n",
    "        \n",
    "        # METHOD 1: Wait for specific elements to load\n",
    "        try:\n",
    "            # Wait for car listing containers to appear\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '[class*=\"CarListing\"], [class*=\"listing\"]')))\n",
    "            print(\"Car listing elements detected!\")\n",
    "        except:\n",
    "            print(\"Waiting for general page elements...\")\n",
    "        \n",
    "        # Scroll to trigger lazy loading of all cars\n",
    "        print(\"\\nScrolling to load all car listings...\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        for scroll_attempt in range(10):  # Maximum 10 scrolls\n",
    "            # Scroll down\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "            \n",
    "            # Check if we've reached the end\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                print(f\"Reached bottom after {scroll_attempt + 1} scrolls\")\n",
    "                break\n",
    "            \n",
    "            last_height = new_height\n",
    "            print(f\"Scroll {scroll_attempt + 1}: Page height increased\")\n",
    "        \n",
    "        # Wait a bit more for any final loading\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # NOW extract the car links\n",
    "        print(\"\\nExtracting car links...\")\n",
    "        \n",
    "        # METHOD A: Look for links with the pattern we know\n",
    "        print(\"\\nA. Searching for '/buy-used-cars/' in all links...\")\n",
    "        all_elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        \n",
    "        for element in all_elements:\n",
    "            try:\n",
    "                href = element.get_attribute('href')\n",
    "                if href and '/buy-used-cars/' in href and '/kolkata/' in href:\n",
    "                    if href not in car_links:\n",
    "                        car_links.append(href)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"Found {len(car_links)} links with pattern matching\")\n",
    "        \n",
    "        # METHOD B: Look for specific class names from your screenshot\n",
    "        print(\"\\nB. Searching for specific CSS classes...\")\n",
    "        \n",
    "        # Try different possible class patterns\n",
    "        class_patterns = [\n",
    "            'a[class*=\"ListingBrandModelDetail\"]',\n",
    "            'a[class*=\"makeModelLink\"]',\n",
    "            'div[class*=\"CarListing\"] a',\n",
    "            'h3[class*=\"ListingBrandModel\"] a'\n",
    "        ]\n",
    "        \n",
    "        for pattern in class_patterns:\n",
    "            try:\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, pattern)\n",
    "                print(f\"Pattern '{pattern}': Found {len(elements)} elements\")\n",
    "                \n",
    "                for element in elements:\n",
    "                    try:\n",
    "                        href = element.get_attribute('href')\n",
    "                        if href and href not in car_links:\n",
    "                            car_links.append(href)\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # METHOD C: Save the FULL rendered HTML for manual inspection\n",
    "        print(\"\\nC. Saving fully rendered HTML for analysis...\")\n",
    "        rendered_html = driver.page_source\n",
    "        \n",
    "        with open('spinny_FULLY_RENDERED.html', 'w', encoding='utf-8') as f:\n",
    "            f.write(rendered_html)\n",
    "        print(\"Saved fully rendered HTML to 'spinny_FULLY_RENDERED.html'\")\n",
    "        \n",
    "        # Count car listings in the rendered HTML\n",
    "        car_count = rendered_html.count('/buy-used-cars/')\n",
    "        print(f\"Found '{/buy-used-cars/}' {car_count} times in rendered HTML\")\n",
    "        \n",
    "        return car_links\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\nBrowser closed.\")\n",
    "\n",
    "def save_and_display_links(car_links):\n",
    "    \"\"\"Save links to file and display results\"\"\"\n",
    "    \n",
    "    if not car_links:\n",
    "        print(\"\\nâŒ No car links found!\")\n",
    "        return\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_links = list(set(car_links))\n",
    "    \n",
    "    # Save to file\n",
    "    filename = 'spinny_kolkata_car_links.txt'\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for link in unique_links:\n",
    "            f.write(link + '\\n')\n",
    "    \n",
    "    print(f\"\\nâœ… SUCCESS! Found {len(unique_links)} unique car listing links!\")\n",
    "    print(f\"Saved to: {filename}\")\n",
    "    \n",
    "    # Display first 10 links\n",
    "    print(\"\\nFirst 10 car listing links:\")\n",
    "    for i, link in enumerate(unique_links[:10], 1):\n",
    "        print(f\"{i}. {link}\")\n",
    "    \n",
    "    # Show link pattern analysis\n",
    "    print(\"\\nðŸ“Š Link Analysis:\")\n",
    "    brands = {}\n",
    "    for link in unique_links:\n",
    "        # Extract brand from URL\n",
    "        parts = link.split('/')\n",
    "        if len(parts) > 5:\n",
    "            brand = parts[5]  # Usually the brand is at this position\n",
    "            brands[brand] = brands.get(brand, 0) + 1\n",
    "    \n",
    "    for brand, count in sorted(brands.items()):\n",
    "        print(f\"  {brand}: {count} cars\")\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SPINNY KOLKATA CAR LINK SCRAPER\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    links = scrape_spinny_car_links()\n",
    "    save_and_display_links(links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
