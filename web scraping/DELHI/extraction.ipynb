{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de624e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup \n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbe54c",
   "metadata": {},
   "source": [
    "## This code is for extracting the car links from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b66edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected: 58\n",
      "Collected: 92\n",
      "Collected: 140\n",
      "Collected: 173\n",
      "Collected: 225\n",
      "Collected: 283\n",
      "Collected: 345\n",
      "Collected: 398\n",
      "Collected: 452\n",
      "Collected: 510\n",
      "Collected: 552\n",
      "Collected: 598\n",
      "Collected: 649\n",
      "Collected: 703\n",
      "Collected: 748\n",
      "Collected: 799\n",
      "Collected: 829\n",
      "Collected: 885\n",
      "Collected: 928\n",
      "Collected: 976\n",
      "Collected: 1024\n",
      "Collected: 1068\n",
      "Collected: 1101\n",
      "Collected: 1101\n",
      "Collected: 1101\n",
      "Collected: 1101\n",
      "Done! Final unique links: 1101\n"
     ]
    }
   ],
   "source": [
    "# def get_browser():\n",
    "#     return Browser('chrome')\n",
    "\n",
    "# browser = get_browser()\n",
    "# target_city = \"delhi\"\n",
    "# output_file = f\"car_links_{target_city}.txt\"\n",
    "# url = f\"https://www.cardekho.com/used-cars+in+{target_city}\"\n",
    "\n",
    "# try:\n",
    "#     browser.visit(url)\n",
    "#     time.sleep(3) # Initial load\n",
    "\n",
    "#     extracted_links = set()\n",
    "#     no_change_count = 0\n",
    "\n",
    "#     while no_change_count < 3:\n",
    "#         previous_count = len(extracted_links)\n",
    "        \n",
    "#         # 1. Faster, snappy scrolling\n",
    "#         # We use a smaller sleep (0.5 to 0.8) between small scrolls\n",
    "#         for _ in range(8):\n",
    "#             browser.execute_script(\"window.scrollBy(0, 1500);\")\n",
    "#             time.sleep(random.uniform(0.5, 0.8)) \n",
    "        \n",
    "#         # 2. Faster \"View More\" check\n",
    "#         # .is_element_present_by_text with a low wait_time is very fast\n",
    "#         if browser.is_element_present_by_text('View More Cars', wait_time=0.5):\n",
    "#             try:\n",
    "#                 browser.find_by_text('View More Cars').click()\n",
    "#                 # Short wait after click for the DOM to update\n",
    "#                 time.sleep(1.5) \n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#         # 3. Quick Extraction\n",
    "#         page_soup = soup(browser.html, 'html.parser')\n",
    "#         car_containers = page_soup.find_all('div', class_=['titlebox', 'hover'])\n",
    "        \n",
    "#         for container in car_containers:\n",
    "#             link_tag = container.find('a', href=True)\n",
    "#             if link_tag:\n",
    "#                 full_link = f\"https://www.cardekho.com{link_tag['href']}\".split('?')[0]\n",
    "#                 if \"/used-car-details/\" in full_link and full_link not in extracted_links:\n",
    "#                     extracted_links.add(full_link)\n",
    "#                     with open(output_file, \"a\") as f:\n",
    "#                         f.write(full_link + \"\\n\")\n",
    "\n",
    "#         current_count = len(extracted_links)\n",
    "#         print(f\"Collected: {current_count}\")\n",
    "\n",
    "#         # 4. Smart Exit\n",
    "#         if current_count > previous_count:\n",
    "#             no_change_count = 0\n",
    "#         else:\n",
    "#             no_change_count += 1\n",
    "#             # If no progress, wait a tiny bit longer once to see if it's just lag\n",
    "#             time.sleep(2)\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "# finally:\n",
    "#     browser.quit()\n",
    "#     print(f\"Done! Final unique links: {len(extracted_links)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea56aaa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07436e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1/1101: https://www.cardekho.com/used-car-details/used-Land-rover-range-rover-30-diesel-lwb-vogue-cars-Gurgaon_e77c1e2c-af8c-4db4-858f-18a99797dc7b.htm\n",
      "Saved: ₹1.40 Crore | 51 specs.\n",
      "Scraping 2/1101: https://www.cardekho.com/used-car-details/used-Kia-seltos-x-line-turbo-dct-cars-Greater-Noida_3b9d9e62-0e20-41f7-95c9-dfe6779ca8b4.htm\n",
      "WARNING: Data missing. Check if page loaded correctly.\n",
      "Scraping 3/1101: https://www.cardekho.com/used-car-details/used-Hyundai-grand-i10-nios-magna-cars-New-Delhi_d76d88ed-f1a3-468c-b61e-e791d4a79a4a.htm\n",
      "WARNING: Data missing. Check if page loaded correctly.\n",
      "Scraping 4/1101: https://www.cardekho.com/used-car-details/used-Kia-seltos-gtx-plus-dct-cars-Gurgaon_c18b1275-3f1f-4d98-aeb2-4ad8ada1d907.htm\n",
      "Saved: ₹8.50 Lakh | 42 specs.\n",
      "Scraping 5/1101: https://www.cardekho.com/used-car-details/used-Kia-sonet-htx-turbo-imt-bsvi-cars-Greater-Noida_32af021a-a93b-4165-9de8-5509abb9a2ab.htm\n",
      "Saved: ₹6.65 Lakh | 42 specs.\n",
      "Scraping 6/1101: https://www.cardekho.com/used-car-details/used-Mahindra-thar-lx-4wd-hard-top-bsvi-cars-New-Delhi_1b0de125-4ca7-41c9-8de4-afbb712af27c.htm\n",
      "Saved: ₹10.83 Lakh | 45 specs.\n",
      "Scraping 7/1101: https://www.cardekho.com/used-car-details/used-Hyundai-creta-e-diesel-cars-Noida_4e5ac1db-362d-43d5-aeef-bc6b5c8c32e6.htm\n",
      "Saved: ₹13 Lakh | 44 specs.\n",
      "Scraping 8/1101: https://www.cardekho.com/used-car-details/used-Hyundai-i20-asta-12-cars-Gurgaon_d05304b1-c6ee-4ce1-ad2c-d86cb8b0d1ea.htm\n",
      "Saved: ₹5.50 Lakh | 52 specs.\n",
      "Scraping 9/1101: https://www.cardekho.com/used-car-details/used-Tata-nexon-ev-xz-plus-lux-cars-New-Delhi_94fddfa4-e238-4478-a968-29fd85453ac3.htm\n",
      "Saved: ₹9.25 Lakh | 47 specs.\n",
      "Scraping 10/1101: https://www.cardekho.com/used-car-details/used-Maruti-ertiga-zxi-at-petrol-cars-Gurgaon_2678fa7b-67cf-4910-bea8-fdfe9edcd29b.htm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m time.sleep(random.uniform(\u001b[32m.5\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m     19\u001b[39m browser.execute_script(\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollTo(0, 600);\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 2. Expansion Logic\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def get_browser():\n",
    "    return Browser('chrome')\n",
    "\n",
    "# 1. Load your links\n",
    "with open(\"car_links_delhi.txt\", \"r\") as f:\n",
    "    all_links = [line.strip() for line in f.readlines()]\n",
    "\n",
    "browser = get_browser()\n",
    "output_file = \"car_dataset_delhi.json\"\n",
    "\n",
    "for index, link in enumerate(all_links):\n",
    "    try:\n",
    "        print(f\"Scraping {index+1}/{len(all_links)}: {link}\")\n",
    "        browser.visit(link)\n",
    "        \n",
    "        # --- Adjusted Timing ---\n",
    "        # Increased initial wait to 1.5-2s to ensure the button and price load\n",
    "        time.sleep(random.uniform(.5, 1))\n",
    "        browser.execute_script(\"window.scrollTo(0, 600);\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # 2. Expansion Logic\n",
    "        try:\n",
    "            view_all_spec_btn = browser.find_by_text('View all Specifications')\n",
    "            if view_all_spec_btn:\n",
    "                browser.execute_script(\"arguments[0].click();\", view_all_spec_btn.first._element)\n",
    "                # Reduced wait slightly to 1.5s for speed, usually enough for text\n",
    "                time.sleep(1) \n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 3. Scrape with BeautifulSoup\n",
    "        page_soup = soup(browser.html, 'html.parser')\n",
    "        car_data = {\"url\": link}\n",
    "        \n",
    "        # --- PRICE EXTRACTION ---\n",
    "        # Targeting the div and span seen in your previous screenshots\n",
    "        price_div = page_soup.find('div', class_='vehiclePrice')\n",
    "        if price_div:\n",
    "            price_span = price_div.find('span')\n",
    "            if price_span:\n",
    "                car_data[\"Price\"] = price_span.get_text(strip=True)\n",
    "\n",
    "        # 4. Extract Labels and Values\n",
    "        spec_items = page_soup.find_all('li', class_='gsc_col-xs-12')\n",
    "        for item in spec_items:\n",
    "            label_tag = item.find('div', class_='label')\n",
    "            value_tag = item.find('span', class_='value-text')\n",
    "            if label_tag and value_tag:\n",
    "                car_data[label_tag.get_text(strip=True)] = value_tag.get_text(strip=True)\n",
    "\n",
    "        # 5. Safety Check & Save\n",
    "        if len(car_data) > 2: # URL + Price + at least one spec\n",
    "            with open(output_file, \"a\") as out:\n",
    "                out.write(json.dumps(car_data) + \"\\n\")\n",
    "            print(f\"Saved: {car_data.get('Price', 'N/A')} | {len(car_data)-2} specs.\")\n",
    "        else:\n",
    "            print(\"WARNING: Data missing. Check if page loaded correctly.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at {link}: {e}\")\n",
    "        browser.quit()\n",
    "        browser = get_browser()\n",
    "        time.sleep(1) \n",
    "        continue\n",
    "\n",
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
